{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/peteray-dev/Food_Hybrid_Recommender_System/blob/master/MSc_Food_Hybrid_Recommeder_system.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dI2wTcURd0O1"
      },
      "source": [
        "### Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m2opX5iyvy4M"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY')\n",
        "os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bn0QizSRwBuS"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d irkaal/foodcom-recipes-and-reviews\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBotEGu7wqhX"
      },
      "outputs": [],
      "source": [
        "!mkdir Food-recipe-and-review.zip\n",
        "\n",
        "!mv /content/foodcom-recipes-and-reviews.zip* /content/Food-recipe-and-review.zip\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QRgauXru2dGR"
      },
      "outputs": [],
      "source": [
        "! unzip /content/Food-recipe-and-review.zip/foodcom-recipes-and-reviews.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7RX8Oox439cc"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade scikit-learn==1.5.0 -q\n",
        "!pip install sentence-transformers -q\n",
        "!pip install --upgrade scikit-learn==1.4.0 -q\n",
        "!pip install networkx -q\n",
        "!pip install torch_geometric -q\n",
        "!pip install captum -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bqt9G56F40Cs"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "# this makes is easier for getting dataframes by default as input/output of\n",
        "# sklearn pipelines\n",
        "sklearn.set_config(transform_output=\"pandas\")\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import ast\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "sns.set(\n",
        "    { \"figure.figsize\": (6, 4) },\n",
        "    style='ticks',\n",
        "    color_codes=True,\n",
        "    font_scale=0.8\n",
        ")\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZq__c0z-Z5g"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import sigmoid_kernel, cosine_similarity\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import Tensor\n",
        "from torch.nn import Embedding, ModuleList, Linear\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch_geometric\n",
        "import torch_geometric.nn as pyg_nn\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.transforms import RandomLinkSplit\n",
        "from torch_geometric.nn import GCNConv, GATConv, SAGEConv, to_hetero\n",
        "from torch.nn.modules.loss import _Loss\n",
        "\n",
        "from torch_geometric.nn.conv import LGConv, GATConv, SAGEConv\n",
        "from torch_geometric.typing import Adj, OptTensor, SparseTensor\n",
        "from torch_geometric.explain import Explainer, GNNExplainer, CaptumExplainer\n",
        "\n",
        "# from torch_lr_finder import LRFinder\n",
        "\n",
        "# This is for the progress bar.\n",
        "from tqdm.auto import tqdm\n",
        "# This is for ploting\n",
        "# import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kweb4D1A48Av"
      },
      "source": [
        "###Importing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RA1cQJ5F43C7"
      },
      "outputs": [],
      "source": [
        "recipes = pd.read_csv('/content/recipes.csv')\n",
        "reviews = pd.read_csv('/content/reviews.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nY4EvvVi5K04"
      },
      "outputs": [],
      "source": [
        "# recipes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIE5XkS45NEc"
      },
      "outputs": [],
      "source": [
        "# reviews"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaF1-_Is0zbU"
      },
      "source": [
        "Analysing Recipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdeZyYKC5Ohb"
      },
      "outputs": [],
      "source": [
        "recipes.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mROnY_GWA6c6"
      },
      "outputs": [],
      "source": [
        "recipes.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JX3HKBw74kr8"
      },
      "outputs": [],
      "source": [
        "# recipes.transpose()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fs2HgxRwFMVV"
      },
      "outputs": [],
      "source": [
        "# recipes.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWkZfbvgISfw"
      },
      "outputs": [],
      "source": [
        "recipes.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cEQbk4Zj7CbI"
      },
      "outputs": [],
      "source": [
        "recipes_cp=recipes.copy()\n",
        "\n",
        "del recipes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HU3QfaLr5eph"
      },
      "outputs": [],
      "source": [
        "recipes_cp['PrepTime']=recipes_cp['PrepTime'].astype(str)\n",
        "recipes_cp['CookTime']=recipes_cp['CookTime'].astype(str)\n",
        "recipes_cp['TotalTime']=recipes_cp['TotalTime'].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MDfSVq8v0stE"
      },
      "outputs": [],
      "source": [
        "#extracting the time utilized to make the food\n",
        "import re\n",
        "def duration_iso(duration):\n",
        "    pattern = re.compile(r'PT(?:(\\d+)H)?(?:(\\d+)M)?')\n",
        "    match = pattern.match(duration)\n",
        "    if not match:\n",
        "        return 0\n",
        "    hours = int(match.group(1)) if match.group(1) else 0\n",
        "    minutes = int(match.group(2)) if match.group(2) else 0\n",
        "    return hours * 60 + minutes\n",
        "\n",
        "recipes_cp['TotalTime'] = recipes_cp['TotalTime'].apply(duration_iso)\n",
        "recipes_cp['PrepTime'] = recipes_cp['PrepTime'].apply(duration_iso)\n",
        "recipes_cp['CookTime'] = recipes_cp['CookTime'].apply(duration_iso)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BSm93EzF79y9"
      },
      "outputs": [],
      "source": [
        "recipes_cp.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TRAk8Hj8cXV"
      },
      "outputs": [],
      "source": [
        "# recipes_cp.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEXc2qdb84S9"
      },
      "outputs": [],
      "source": [
        "recipes_cp['ReviewCount'].max()\n",
        "\n",
        "recipes_cp[['RecipeId', 'Name']][recipes_cp['ReviewCount']==3063]\n",
        "#Lets check if the number of count for the review is the same as the one in review\n",
        "#the id is 45809 with count of 3063"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGNP0OzWEPpQ"
      },
      "outputs": [],
      "source": [
        "recipes_cp['ReviewCount'].min()\n",
        "recipes_cp[['RecipeId', 'Name']][recipes_cp['ReviewCount']==1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yc3V_I6Flt2"
      },
      "outputs": [],
      "source": [
        "#getting the recipeID of all with Null\n",
        "recipes_cp[['RecipeId', 'ReviewCount']].loc[recipes_cp['ReviewCount'].isnull()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPcM1WS1GU4K"
      },
      "outputs": [],
      "source": [
        "# let's check for the number of review count for the recipe Id 38, which should be 4 as the same as the review data frame(check below)\n",
        "\n",
        "recipes_cp[['RecipeId','ReviewCount']].loc[recipes_cp['RecipeId']==38]\n",
        "\n",
        "#This reveals that the the reviewcount in recipe dataframe, is gotten from the review data frame,"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrjB25_6DGxk"
      },
      "source": [
        "### Basic work on review dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PQOsna0Cx-O"
      },
      "outputs": [],
      "source": [
        "reviews.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iXfvy7P1C6Cv"
      },
      "outputs": [],
      "source": [
        "#Let's remove the ones without reviews\n",
        "reviews.dropna(subset='Review', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3EYxjCKiDfLd"
      },
      "outputs": [],
      "source": [
        "reviews.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UFRZU4q_A9P3"
      },
      "outputs": [],
      "source": [
        "reviews[reviews['RecipeId']==45809]\n",
        "# Inside the review dataframe, the id with the max review count from the recipe dataframe has 2892\n",
        "# that's just a difference of (3063 - 2892)=71"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zR29mt2SELGP"
      },
      "outputs": [],
      "source": [
        "reviews[reviews['RecipeId']==53]\n",
        "# From Analysis, it is seen that the count correlates with what is in the recipes['reviewcount']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFygrxS6EK8Q"
      },
      "outputs": [],
      "source": [
        "review_count = reviews.groupby('RecipeId').size().reset_index(name='ReviewCount')\n",
        "# review_count"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJrdF5F2RbnE"
      },
      "outputs": [],
      "source": [
        "# Based on the above analysis, it is seen that tsome recipesId were not reviewed nor rate as well\n",
        "\n",
        "# i will remove Aggregaterating and ratingcount\n",
        "# Aggregate rating is a phrase that is commonly used to talk about an average score or rating,\n",
        "# which is calculated from many individual reviews. It gives a brief summary of different opinions\n",
        "#  while rating count only tells the number of recipe id that was rated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TE5ydP0PNOh-"
      },
      "outputs": [],
      "source": [
        "recipes_cp.drop(columns=['AggregatedRating', 'ReviewCount', 'RecipeYield', 'RecipeServings', 'Images'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K2WjGWVzCVAg"
      },
      "outputs": [],
      "source": [
        "reviews.drop(columns=['DateSubmitted', 'DateModified'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1fdpPL9PC6K4"
      },
      "outputs": [],
      "source": [
        "#Merging the 2 dataset together (review and recipe)\n",
        "Merged_df = pd.merge(recipes_cp,reviews, how=\"outer\", left_on = 'RecipeId', right_on= 'RecipeId')\n",
        "Merged_df.info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y9I0yjLZEWcs"
      },
      "outputs": [],
      "source": [
        "# Merge AuthorId_x and AuthorId_y and also Merge AuthorName_x and AuthorName_y\n",
        "Merged_df['AuthorId'] = Merged_df['AuthorId_x'].combine_first(Merged_df['AuthorId_y'])\n",
        "Merged_df['AuthorName'] = Merged_df['AuthorName_x'].combine_first(Merged_df['AuthorName_y'])\n",
        "\n",
        "# Drop the original columns\n",
        "Merged_df.drop(columns=['AuthorId_x', 'AuthorId_y', 'AuthorName_x', 'AuthorName_y'], inplace=True)\n",
        "\n",
        "\n",
        "Merged_df.info()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xSF1iGjFE5o"
      },
      "outputs": [],
      "source": [
        "Merged_df.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_GhrXE92Ezkk"
      },
      "outputs": [],
      "source": [
        "# Merged_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOu0h7kFFik5"
      },
      "outputs": [],
      "source": [
        "Merged_df['RecipeIngredientParts'][1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZSXuu-MTLBp"
      },
      "outputs": [],
      "source": [
        "def convert_to_list(string):\n",
        "    # Remove the leading 'c(' and trailing ')'\n",
        "    string = re.sub(r'^c\\(|\\)$','', string)\n",
        "    # Split by ', ' and remove surrounding quotes\n",
        "    ingredients = [item.strip().strip('\"') for item in string.split(', ')]\n",
        "    return ingredients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGIpfXerLlYB"
      },
      "outputs": [],
      "source": [
        "Merged_df['RecipeIngredientParts'] = Merged_df['RecipeIngredientParts'].apply(convert_to_list)\n",
        "Merged_df['RecipeInstructions'] = Merged_df['RecipeInstructions'].apply(convert_to_list)\n",
        "Merged_df['Keywords'] = Merged_df['Keywords'].apply(convert_to_list)\n",
        "\n",
        "#All columns converted to List\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83CxVnavTu9I"
      },
      "source": [
        "### Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D__lQEQ4NM3S"
      },
      "outputs": [],
      "source": [
        "num_col = ['CookTime', 'PrepTime', 'TotalTime', 'Calories', 'FatContent', 'SaturatedFatContent', 'CholesterolContent',\n",
        "           'SodiumContent', 'CarbohydrateContent', 'FiberContent', 'SugarContent', 'ProteinContent']\n",
        "\n",
        "fig, axis = plt.subplots(4,3, figsize=(15,12))\n",
        "axis = axis.ravel()\n",
        "for i, ax in enumerate(axis):\n",
        "  sns.boxplot(data=Merged_df[num_col[i]], ax=ax)\n",
        "  ax.set(title=num_col[i])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TU5X58PT0OA"
      },
      "source": [
        "There are a lot of outliers in this data set, and indicating they aren't normally distributed as well\n",
        "\n",
        "Now i'm going to remove the outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTRPP5h8RSsV"
      },
      "outputs": [],
      "source": [
        "for col in num_col:\n",
        "  Q3 = Merged_df[col].quantile(0.75)\n",
        "  Q1 = Merged_df[col].quantile(0.25)\n",
        "  IQR =  Q3 - Q1\n",
        "  Merged_df2 = Merged_df[(Merged_df[col]>=Q1-1.5*IQR) & (Merged_df[col]<=Q3+1.5*IQR) ]\n",
        "  print(f'lower_limit for {col}: {Q1-1.5*IQR}, upper_limit{col}: {Q3+1.5*IQR}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bTxTxCVaXDWZ"
      },
      "outputs": [],
      "source": [
        "fig, axis = plt.subplots(4,3, figsize=(15,12))\n",
        "axis = axis.ravel()\n",
        "for i, ax in enumerate(axis):\n",
        "  sns.boxplot(data=Merged_df2[num_col[i]], ax=ax)\n",
        "  ax.set(title=num_col[i])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avlJezkXXM4o"
      },
      "outputs": [],
      "source": [
        "# print(f'the Author unique name')\n",
        "Merged_df2['AuthorName'].nunique() # I can do stratified sampling using RecipeId"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqQQmOoWoqnG"
      },
      "outputs": [],
      "source": [
        "col_unique_list = ['RecipeId', 'Name', 'RecipeCategory', 'Rating', 'AuthorId', 'AuthorName']\n",
        "for col in col_unique_list:\n",
        "    unique_values = Merged_df2[col].nunique()\n",
        "    print(f'The {col} column has {unique_values} unique values')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBoBAxTGoJ9X"
      },
      "outputs": [],
      "source": [
        "Merged_df2['Rating'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruSF1COuqsUk"
      },
      "outputs": [],
      "source": [
        "# lets take a user Id (44642) and see the item he/she interated with\n",
        "# This user rated 29 items,\n",
        "Merged_df2[Merged_df2['AuthorId']==44642]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yAW_dVphsPn"
      },
      "source": [
        "problems:\n",
        "Data sparsity: the problem of having insufficient or missing ratings or interactions between users and items, I will address this by building a model that helps in prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZgxgq3ZAir2"
      },
      "outputs": [],
      "source": [
        "# Merged_df2.columns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUlvgX56iMPh"
      },
      "outputs": [],
      "source": [
        "# Merged_df2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xtd40atHMNn4"
      },
      "source": [
        "### Exploratory Data Analysis 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTdDbt0Y_4WO"
      },
      "outputs": [],
      "source": [
        "recipe_col = ['RecipeId', 'Name', 'AuthorId', 'Description','RecipeCategory', 'Keywords',  'RecipeIngredientParts', 'Calories', 'FatContent',\n",
        "       'SaturatedFatContent', 'CholesterolContent', 'SodiumContent', 'CarbohydrateContent', 'FiberContent', 'SugarContent', 'ProteinContent', 'DatePublished' ,]\n",
        "review_col=['RecipeId','ReviewId', 'AuthorId', 'Rating', 'Review']\n",
        "recipe_df = recipes_cp[recipe_col]\n",
        "review_df = reviews[review_col]\n",
        "\n",
        "\n",
        "del recipes_cp\n",
        "del reviews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2_7Xd0V7lq_"
      },
      "outputs": [],
      "source": [
        "# working with data between 2013 and 2020\n",
        "recipe_df['DatePublished'] = pd.to_datetime(recipe_df['DatePublished'])\n",
        "recipe_df['DatePublished'].dt.year.value_counts()\n",
        "recipe_df = recipe_df[recipe_df['DatePublished'].dt.year.between(2013, 2020)]\n",
        "# recipe_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-PselhBVzN1L"
      },
      "outputs": [],
      "source": [
        "recipe_df['RecipeCategory'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SC12YHMnhr09"
      },
      "outputs": [],
      "source": [
        "# # i will drop the duplicate in the recipe df\n",
        "recipe_df = recipe_df.dropna()\n",
        "recipe_df['Name'].nunique()\n",
        "# recipe_df = recipe_df.sample(50000, random_state=0) # i can seelect them based on time for prepartion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PWeQ9X1Z2LDh"
      },
      "outputs": [],
      "source": [
        "recipe_df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dXg8A5x1YW0M"
      },
      "outputs": [],
      "source": [
        "# num_col = ['Calories', 'FatContent', 'SaturatedFatContent', 'CholesterolContent',\n",
        "#            'SodiumContent', 'CarbohydrateContent', 'FiberContent', 'SugarContent', 'ProteinContent']\n",
        "\n",
        "# fig, axis = plt.subplots(3,3, figsize=(15,12))\n",
        "# axis = axis.ravel()\n",
        "# for i, ax in enumerate(axis):\n",
        "#   sns.boxplot(data=recipe_df[num_col[i]], ax=ax)\n",
        "#   ax.set(title=num_col[i])\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-wdHCV5FLEPS"
      },
      "outputs": [],
      "source": [
        "# Remove outliers\n",
        "num_col = ['Calories', 'FatContent', 'SaturatedFatContent', 'CholesterolContent',\n",
        "           'SodiumContent', 'CarbohydrateContent', 'FiberContent', 'SugarContent', 'ProteinContent']\n",
        "\n",
        "for col in num_col:\n",
        "  Q3 = recipe_df[col].quantile(0.75)\n",
        "  Q1 = recipe_df[col].quantile(0.25)\n",
        "  IQR =  Q3 - Q1\n",
        "  recipe_df = recipe_df[(recipe_df[col]>=Q1-1.5*IQR) & (recipe_df[col]<=Q3+1.5*IQR) ]\n",
        "  print(f'lower_limit for {col}: {Q1-1.5*IQR}, upper_limit{col}: {Q3+1.5*IQR}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HmgiLgM1N8bv"
      },
      "outputs": [],
      "source": [
        "num_col = ['Calories', 'FatContent', 'SaturatedFatContent', 'CholesterolContent',\n",
        "           'SodiumContent', 'CarbohydrateContent', 'FiberContent', 'SugarContent', 'ProteinContent']\n",
        "fig, axis = plt.subplots(3,3, figsize=(15,12))\n",
        "axis = axis.ravel()\n",
        "for i, ax in enumerate(axis):\n",
        "  sns.histplot(data=recipe_df[num_col[i]], ax=ax)\n",
        "  ax.set(title=num_col[i])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3CieDlclOXDW"
      },
      "outputs": [],
      "source": [
        "# recipe_df.nunique() #31088"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fT-X4VEKI2Nb"
      },
      "outputs": [],
      "source": [
        "recipe_df = recipe_df.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9EccjSHH8Kb"
      },
      "outputs": [],
      "source": [
        "recipe_df[recipe_df['Name']=='Chicken Paprikash']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "APBis9-JQ_vi"
      },
      "source": [
        "### Text Preprocessing\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bq3gMWQwO7GS"
      },
      "outputs": [],
      "source": [
        "# recipe_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qGTjy-hbRJ1F"
      },
      "outputs": [],
      "source": [
        "recipe_df['Keywords'].iloc[9]\n",
        "\n",
        "#  the 'c' is used used for concatenation in R"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmUVPqG4SLwo"
      },
      "outputs": [],
      "source": [
        "def convert_to_list(col):\n",
        "  cln = col.replace('c(', '').replace(')', '')\n",
        "  return cln"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32t1ZYo86cpC"
      },
      "outputs": [],
      "source": [
        "recipe_df['Keywords']=recipe_df['Keywords'].apply(convert_to_list)\n",
        "recipe_df['RecipeIngredientParts']=recipe_df['RecipeIngredientParts'].apply(convert_to_list)\n",
        "# recipe_df['Keywords']=recipe_df['Keywords'].apply(convert_to_list).to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEYC9ouG8V07"
      },
      "outputs": [],
      "source": [
        "recipe_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKLNispE-dq0"
      },
      "outputs": [],
      "source": [
        "type(recipe_df['RecipeIngredientParts'][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TAbU232t_td0"
      },
      "outputs": [],
      "source": [
        "#lets convert to list so that i can easily combine the 3 column together then preprocess it\n",
        "recipe_df['Keywords']=recipe_df['Keywords'].apply(eval)\n",
        "# # removing any presenceof '('\n",
        "# recipe_df['RecipeIngredientParts'] = recipe_df['RecipeIngredientParts'].apply(lambda x: re.sub(r'^\\(', '', x) if pd.notna(x) else x)\n",
        "# #convert to list\n",
        "# recipe_df['RecipeIngredientParts']=recipe_df['RecipeIngredientParts'].apply(eval)\n",
        "recipe_df['RecipeIngredientParts']=recipe_df['RecipeIngredientParts'].str.replace('\"', '')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXZKMYslKC95"
      },
      "outputs": [],
      "source": [
        "recipe_df.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-yBZAOK_4EA"
      },
      "outputs": [],
      "source": [
        "# combining 3 columns (Description, Recipecatergory, keywords)\n",
        "recipe_df['Merged_recipe_info'] = recipe_df.apply(\n",
        "    lambda row: f\"{row['Description']} {row['RecipeIngredientParts']} {row['RecipeCategory']}  {', '.join(row['Keywords'])}\",\n",
        "    axis=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iDsfVMSm_-2U"
      },
      "outputs": [],
      "source": [
        "recipe_df['Merged_recipe_info'].iloc[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lvD_JvnWQfnY"
      },
      "source": [
        "lets remove stop words using the natural language processing toolkits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-gaZrl_1QRnX"
      },
      "outputs": [],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "print(stopwords.words('english'))\n",
        "stp=['Food.com.']\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words.update(stp)\n",
        "\n",
        "stop_words\n",
        "\n",
        "# Using this, i might loose the neccessary information, let me go for alaguage processing that will preserve my message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwK2rOvtEf6m"
      },
      "outputs": [],
      "source": [
        "#  now using snowball stemmer (reducing a word to its base word or stem in such a way that the words of similar kind lie under a common stem, e.g run,ran, running can be reduce  to the base word 'run')\n",
        "# snowball_stemmer = SnowballStemmer()\n",
        "# Lemmatizatio also give context to the word and does stemming too, so lets process the merged column using lemmatization instead\n",
        "word_lemma = WordNetLemmatizer()\n",
        "\n",
        "def preprocess(word):\n",
        "  word = str(word).lower()\n",
        "  # let's also replace some meaningful word that i di not want the stop word to remove, e.g '<' with  below\n",
        "  word = word.replace('<', 'lesser than').replace('>', 'greater than').replace(\"Food.com\", \"\").replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\")\\\n",
        "                           .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\")\\\n",
        "                           .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\")\\\n",
        "                           .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\")\\\n",
        "                           .replace(\"%\", \" percent \").replace(\"â‚¬\", \" euro \").replace(\"'ll\", \" will\").replace('Mins', 'minutes')\n",
        "\n",
        "  # i wil also remove any symbols that may be funt\n",
        "  word=re.sub('[^A-Za-z0-9]+', ' ', str(word))\n",
        "  #Applying the lemmatizer\n",
        "  lemma_word = ' '.join(word_lemma.lemmatize(w) for w in word.split() if w not in stop_words)\n",
        "\n",
        "  return lemma_word\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6z5EvIVfk4L"
      },
      "outputs": [],
      "source": [
        "recipe_df['recipe_process_info'] = recipe_df['Merged_recipe_info'].apply(preprocess)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8jS0vr6Ofutc"
      },
      "outputs": [],
      "source": [
        "recipe_df['recipe_process_info'].iloc[10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ta--sWVPgazu"
      },
      "outputs": [],
      "source": [
        "recipe_df['Merged_recipe_info'].iloc[10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvMBRArPg1Gx"
      },
      "outputs": [],
      "source": [
        "# recipe_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5iu-2VCQkZTw"
      },
      "source": [
        "### Content Based Recommendation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Q_IrKpRkgUo"
      },
      "outputs": [],
      "source": [
        "tfvec = TfidfVectorizer(min_df=3, ngram_range=(1,2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkS0o-iFqurK"
      },
      "outputs": [],
      "source": [
        "tfvec_mtrx = tfvec.fit_transform(recipe_df['recipe_process_info'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXj2hMM5q6fF"
      },
      "outputs": [],
      "source": [
        "tfvec_mtrx.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7H_ierh55rdA"
      },
      "source": [
        "Sigmoid Kernel:\n",
        "\n",
        "from sklearn doc: (Note that the tf-idf functionality in sklearn.feature_extraction.text can produce normalized vectors, in which case cosine_similarity is equivalent to linear_kernel, only slower.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rv1Hd8xyrAIH"
      },
      "outputs": [],
      "source": [
        "sig_mat = sigmoid_kernel(tfvec_mtrx, tfvec_mtrx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LhgTuTKQr2Ml"
      },
      "outputs": [],
      "source": [
        "sig_mat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTk0N3Na7npR"
      },
      "outputs": [],
      "source": [
        "ind = pd.Series(recipe_df.index, index=recipe_df['Name'])\n",
        "ind"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WZRx_p8_JfZ"
      },
      "outputs": [],
      "source": [
        "# cos_mat[484727]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaWqsN5P90VT"
      },
      "outputs": [],
      "source": [
        "# getting similar food recipe\n",
        "def get_food_recipe_sig_content_based(indices):\n",
        "  scores = list(enumerate(sig_mat[indices]))\n",
        "  # sorting the similarity scores\n",
        "  scores = sorted(scores, key=lambda x:x[1], reverse=True)\n",
        "  # getting the first 5 most similar food recipe\n",
        "  scores = scores[0:6]\n",
        "  # print(scores)\n",
        "\n",
        "\n",
        "  # getting the indices\n",
        "  ind = [i[0] for i in scores]\n",
        "  # print(ind)\n",
        "  similar_recipe = pd.DataFrame(recipe_df.iloc[ind])\n",
        "  print(f'Recommendation for RecipeID {ind[0]},RecipeId:{recipe_df.iloc[ind[0]][\"RecipeId\"]}, Name:{recipe_df.iloc[ind[0]][\"Name\"]}')\n",
        "\n",
        "  return similar_recipe[1:]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVIGxYR-_9ik"
      },
      "outputs": [],
      "source": [
        "ind['Chicken Paprikash']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZ50QOFZ_9gK"
      },
      "outputs": [],
      "source": [
        "get_food_recipe_sig_content_based(4)\n",
        "# [4, 8806, 17595, 28020, 27679, 7443]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hcg1pFta_9a-"
      },
      "outputs": [],
      "source": [
        "# without the usage of nutritional content, thsi wwas the similarities selected\n",
        "# [4, 8806, 17595, 28020, 27679, 7443]\n",
        "# let's check why it dropped last 3\n",
        "recipe_df[recipe_df.index == 7539 ]\n",
        "# recipe_df[recipe_df.index == 13901 ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Azm1smD5n9y"
      },
      "source": [
        "Cosine Similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5PdHdlCsIVi"
      },
      "outputs": [],
      "source": [
        "cos_mat = cosine_similarity(tfvec_mtrx, tfvec_mtrx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZ89JcHN5OEv"
      },
      "outputs": [],
      "source": [
        "cos_mat[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ad6zk9Iw5Sd0"
      },
      "outputs": [],
      "source": [
        "# cos_sim = list(enumerate(cosine_similarity[436511]))\n",
        "# sorted(cos_mat[0].max())\n",
        "\n",
        "sort = sorted(cos_mat[40])\n",
        "sort[-2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwclJO4C-ou3"
      },
      "outputs": [],
      "source": [
        "# getting similar food recipe\n",
        "def get_food_recipe_cos_content_based(indices):\n",
        "  scores = list(enumerate(cos_mat[indices]))\n",
        "  # sorting the similarity scores\n",
        "  scores = sorted(scores, key=lambda x:x[1], reverse=True)\n",
        "  # getting the first 5 most similar food recipe\n",
        "  scores = scores[0:6]\n",
        "  # print(scores)\n",
        "  ind = [i[0] for i in scores]\n",
        "  similarity_scores = [i[1] for i in scores]\n",
        "  # getting the indices\n",
        "  ind = [i[0] for i in scores]\n",
        "  print(f\"Recommendation for recipe name:{recipe_df.iloc[ind[0]]['Name']}\")\n",
        "  similar_recipe = recipe_df.loc[ind[1:]][['RecipeId', 'Name', 'AuthorId', 'Description', 'RecipeCategory', 'Keywords', 'RecipeIngredientParts', 'Calories']]\n",
        "  similar_recipe['Score'] = similarity_scores[1:]\n",
        "  return similar_recipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JrJkuo4H_Exe"
      },
      "outputs": [],
      "source": [
        "res = get_food_recipe_cos_content_based(4)\n",
        "res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPsNE5isSTxK"
      },
      "source": [
        "### Collaborative Filtering - Item-Item based collaborative"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyUt4XgFSbt3"
      },
      "outputs": [],
      "source": [
        "reviews.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5x_g4PJJSrt4"
      },
      "outputs": [],
      "source": [
        "reviews.nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nHC9kDWxTyIk"
      },
      "outputs": [],
      "source": [
        "reviews_rating_df = (\n",
        "    reviews.groupby('RecipeId')['Rating']\n",
        "    .agg(['mean', 'count'])\n",
        "    .rename(columns={'mean': 'AvgRating', 'count': 'RatingCount'})\n",
        "    .sort_values(by='RatingCount', ascending=False)\n",
        "    .reset_index()  # This line ensures the RecipeId is part of the DataFrame\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ky-mfAXkUlG3"
      },
      "outputs": [],
      "source": [
        "review_df = pd.merge(reviews, reviews_rating_df, left_on='RecipeId', right_on='RecipeId', how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6FPsZ7gYD22"
      },
      "outputs": [],
      "source": [
        "review_df.sort_values(by='RatingCount', ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOXQvZUPbUhD"
      },
      "outputs": [],
      "source": [
        "review_df.nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJIfCPJ3YGR-"
      },
      "outputs": [],
      "source": [
        "# Let's create a rating_df, this df will have 'RecipeId', 'AuthorId', 'AvgRating', 'RatingCount\n",
        "\n",
        "rating_df = review_df[['RecipeId', 'AuthorId','Rating', 'AvgRating', 'RatingCount']]\n",
        "rating_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhYEpjLNcBUg"
      },
      "outputs": [],
      "source": [
        "rating_df.drop_duplicates(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CR3Pm1Bcd5H"
      },
      "outputs": [],
      "source": [
        "rating_df.sort_values(by='RatingCount', ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_fYC647GhaCA"
      },
      "outputs": [],
      "source": [
        "rating_df.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZiWaEwWdgVE"
      },
      "outputs": [],
      "source": [
        "col = ['Rating', 'AvgRating', 'RatingCount']\n",
        "\n",
        "fig, axis = plt.subplots(3,1, figsize=(10,8))\n",
        "axis = axis.ravel()\n",
        "for i, ax in enumerate(axis):\n",
        "  sns.histplot(data=rating_df[col[i]], ax=ax, kde=True)\n",
        "  ax.set(title=col[i])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "# for x in col:\n",
        "#   sns.histplot(x=rating_df[x], kde=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yOUS-rdgh4Fx"
      },
      "outputs": [],
      "source": [
        "# I will prefer to remove the lesser count of ratingcount\n",
        "# let's select the recipeId with the highest numbe rof ratiing, indicating that they are the most popular recipes\n",
        "# that most user interact with and this will be a better use for recommendation, and this will best solve the\n",
        "# problem of cold start by suggesting popular item to the new user\n",
        "\n",
        "rating_df = rating_df[rating_df['RatingCount']>=100].sort_values(by=\"RecipeId\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEivV5X38_8E"
      },
      "outputs": [],
      "source": [
        "rating_df.nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZxQ8s9A2vIT"
      },
      "outputs": [],
      "source": [
        "# user-item matrix\n",
        "rating_matrix = rating_df.pivot(index='AuthorId', columns='RecipeId', values='Rating').fillna(0)\n",
        "\n",
        "rating_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J9rF5Wkx8D3t"
      },
      "outputs": [],
      "source": [
        "rating_matrix.shape\n",
        "# users = 86723\n",
        "# recipe = 1065"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPo9j9jf-0LA"
      },
      "source": [
        "Recommendation using pearson correlation (Item-based Collaborative Filtering)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PaUpUtEK9UlY"
      },
      "outputs": [],
      "source": [
        "corr_matrix = rating_matrix.corr(method='pearson')\n",
        "\n",
        "corr_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "czdYBL4JETZj"
      },
      "outputs": [],
      "source": [
        "new = rating_matrix.loc[1535]\n",
        "new[new>0]\n",
        "# this user only rated 81 items so 984 items was not rated by this user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwlsRcDfIqX-"
      },
      "outputs": [],
      "source": [
        "corr_matrix[56].sort_values()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snvAioak_52B"
      },
      "outputs": [],
      "source": [
        "def recommend_item_pearson(user_id, user_item_matrix, item_corr_matrix, n_recomm):\n",
        "  #select the user's rating\n",
        "  rating = user_item_matrix.loc[user_id]\n",
        "\n",
        "  # recipe not interacted with\n",
        "  unrated = rating[rating==0].index\n",
        "\n",
        "  score={}\n",
        "  for item in unrated:\n",
        "    # let's get the correlation of the unrated recipe by the user with other items\n",
        "    item_corr = item_corr_matrix[item]\n",
        "\n",
        "    # getting the rated item index by user\n",
        "    rated_item = rating[rating>0].index\n",
        "    # getting the correlation for rated items\n",
        "    item_corr = item_corr[rated_item]\n",
        "    # getting user rating for rated items\n",
        "    similar_rating = rating[rated_item]\n",
        "\n",
        "    # calculation of recommendation scores using weighted average of similar items\n",
        "    num = sum(similar_rating*item_corr)\n",
        "    denom = sum(item_corr)\n",
        "\n",
        "    if denom !=0:\n",
        "      score[item] = num/denom\n",
        "\n",
        "  recommended_items = sorted(score.items(), key=lambda x:x[1], reverse=True)[:n_recomm]\n",
        "\n",
        "  recommended_indices = [item[0] for item in recommended_items]\n",
        "\n",
        "  recommended_scores = [item[1] for item in recommended_items]\n",
        "\n",
        "  recommended_recipes = pd.DataFrame({\n",
        "      'RecipeId': recommended_indices,\n",
        "      'Score': recommended_scores  # Using values() to get scores from dictionary\n",
        "  })\n",
        "  print(f\"\"\"The recommended recipes for user with ID {user_id}: {reviews[reviews['AuthorId'] == user_id]['AuthorName'].tolist()[0]}\"\"\")\n",
        "  rp = pd.merge(recommended_recipes, recipes[['Name', 'RecipeIngredientParts', 'RecipeId']], on='RecipeId')\n",
        "  rp\n",
        "  return rp\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CkbHEF8LUfW"
      },
      "outputs": [],
      "source": [
        "user_id = 1535\n",
        "n_recomm= 10\n",
        "recommend = recommend_item_pearson(user_id, rating_matrix, corr_matrix, n_recomm)\n",
        "recommend"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRLsYTMXwvRN"
      },
      "source": [
        "### Collaborative Filtering User-Item using KNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPA_Rlc07Ysa"
      },
      "source": [
        "The prediction of recipe is based on user interaction, this method predict recipe by comparing what the user has interated with with user with similar interation, so it then predict what similar users have predicted that the user has not predicted.\n",
        "It can de deduced that the user and similar users have rated similar items in the past\n",
        "\n",
        "The KNN model finds recipes that have similar user ratings to this recipe.\n",
        "\n",
        "User Ratings: The similarity between recipes is based on how users rated them. If two recipes are rated similarly by many users, they are considered similar.\n",
        "\n",
        "Cosine Similarity: This metric measures the angle between two rating vectors (one for each recipe). Smaller angles (closer to zero) mean the recipes are more similar.\n",
        "\n",
        "KNN Model: This model identifies the nearest neighbors (similar recipes) based on the cosine similarity of their ratings.\n",
        "\n",
        "Print Recommendations: The model prints out the names of these similar recipes, along with a similarity score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cypa91RlMGAv"
      },
      "outputs": [],
      "source": [
        "combined_df=pd.merge(rating_df, recipes[['RecipeId', 'Name']], on='RecipeId' )\n",
        "combined_df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Htw3jTCMS8H_"
      },
      "outputs": [],
      "source": [
        "combined_df.nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGR2aKF_ZdKs"
      },
      "outputs": [],
      "source": [
        "combined_df_pivot = combined_df.pivot(index='Name', columns='AuthorId', values='Rating').fillna(0)\n",
        "# the matrix is highhly sparse, so i can introduce csr sparse matrix\n",
        "combined_df_matrix = csr_matrix(combined_df_pivot.values)\n",
        "combined_df_matrix.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJ7ACNIP7SBj"
      },
      "outputs": [],
      "source": [
        "combined_df_pivot\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWg3UnFnpymw"
      },
      "outputs": [],
      "source": [
        "print(\"Shape of sparse matrix:\", combined_df_matrix.shape)\n",
        "print(\"Nonzero values:\", combined_df_matrix.data)\n",
        "print(\"Column indices of nonzero values:\", combined_df_matrix.indices)\n",
        "print(\"Index pointer array:\", combined_df_matrix.indptr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9TLX0p_ipF2"
      },
      "outputs": [],
      "source": [
        "knn = NearestNeighbors(metric='cosine', algorithm='brute')\n",
        "knn.fit(combined_df_matrix)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXxUo9WtnQqT"
      },
      "outputs": [],
      "source": [
        "query_index = np.random.choice(combined_df_pivot.shape[0])\n",
        "print(query_index)\n",
        "# combined_df_pivot[1533]\n",
        "# comb = combined_df_pivot.transpose()\n",
        "# comb\n",
        "combined_df_pivot.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HqufJCIXqEuy"
      },
      "outputs": [],
      "source": [
        "distance, indices = knn.kneighbors(combined_df_pivot.iloc[query_index,:].values.reshape(1,-1), n_neighbors=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twgmHBhEu2wb"
      },
      "outputs": [],
      "source": [
        "distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-u7ag-3rAxs"
      },
      "outputs": [],
      "source": [
        "for i in range(0,len(distance.flatten())):\n",
        "  if i==0:\n",
        "    print(f'Recommmendation for {combined_df_pivot.index[query_index]}')\n",
        "  else:\n",
        "    print(f'{i}: {combined_df_pivot.index[indices.flatten()[i]]} has a distance of {distance.flatten()[i]} ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULJqeOAJ8J6F"
      },
      "outputs": [],
      "source": [
        "print(f'let us get the other users that also rated {combined_df_pivot.index[query_index]} '  )\n",
        "combined_df_pivot.iloc[query_index, :][combined_df_pivot.iloc[query_index, :] > 0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVarRgoAr-oI"
      },
      "outputs": [],
      "source": [
        "def recommend_knn(matrix, pivot_table, query_index, n_neighbors=10):\n",
        "  knn = NearestNeighbors(metric='cosine', algorithm='brute')\n",
        "  knn.fit(matrix)\n",
        "\n",
        "  distance, indices = knn.kneighbors(pivot_table.iloc[query_index,:].values.reshape(1,-1), n_neighbors=n_neighbors)\n",
        "  # print(distance)\n",
        "  for i in range(0,len(distance.flatten())):\n",
        "    if i==0:\n",
        "      print(f'Recommmendation for {pivot_table.index[query_index]}')\n",
        "    else:\n",
        "      print(f'{i}: {pivot_table.index[indices.flatten()[i]]} has a distance of {distance.flatten()[i]} ')\n",
        "\n",
        "  return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZ3NHOgTtwVq"
      },
      "outputs": [],
      "source": [
        "# query_index = np.random.choice(combined_df_pivot.shape[0])\n",
        "\n",
        "recommend_knn(combined_df_matrix, combined_df_pivot, query_index)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gy-GEwUvtzb"
      },
      "outputs": [],
      "source": [
        "recipes[recipes['Name']=='Crock Pot Ravioli']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G-rGiiB11gfX"
      },
      "outputs": [],
      "source": [
        "def recommend_knn(matrix, pivot_table, query_index, n_neighbors=10):\n",
        "    knn = NearestNeighbors(metric='cosine', algorithm='brute')\n",
        "    knn.fit(matrix)\n",
        "\n",
        "    distance, indices = knn.kneighbors(pivot_table.iloc[query_index, :].values.reshape(1, -1), n_neighbors=n_neighbors)\n",
        "\n",
        "    for i in range(0, len(distance.flatten())):\n",
        "        if i == 0:\n",
        "            print(f'Recommendation for {pivot_table.index[query_index]}')\n",
        "        else:\n",
        "            print(f'{i}: {pivot_table.index[indices.flatten()[i]]} has a distance of {distance.flatten()[i]}')\n",
        "\n",
        "    # Dimensionality Reduction using PCA\n",
        "    pca = PCA(n_components=2, svd_solver='arpack') # Set the svd_solver to 'arpack' to handle sparse matrices\n",
        "    matrix_2d = pca.fit_transform(matrix)\n",
        "    matrix_2d_df = pd.DataFrame(matrix_2d, columns=['pca0', 'pca1'])\n",
        "\n",
        "    # Get the 2D coordinates for the query point and its neighbors\n",
        "    query_2d = matrix_2d_df.iloc[query_index]\n",
        "    neighbors_2d = matrix_2d_df.iloc[indices.flatten()]\n",
        "\n",
        "    # Plotting\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    # Plot all data points\n",
        "    # plt.scatter(matrix_2d_df['pca0'], matrix_2d_df['pca1'], c='lightgray', label='All Points')\n",
        "\n",
        "    # Plot the query point\n",
        "    plt.scatter(query_2d['pca0'], query_2d['pca1'], c='red', label='Query Point')\n",
        "\n",
        "    # Plot the nearest neighbors (excluding the query point itself)\n",
        "    plt.scatter(neighbors_2d['pca0'][1:], neighbors_2d['pca1'][1:], c='blue', label='Nearest Neighbors')\n",
        "\n",
        "    for i in range(1, len(distance.flatten())):\n",
        "        plt.annotate(f'{i}', (neighbors_2d['pca0'].iloc[i], neighbors_2d['pca1'].iloc[i]))\n",
        "\n",
        "    plt.xlabel('PCA Component 1')\n",
        "    plt.ylabel('PCA Component 2')\n",
        "    plt.legend()\n",
        "    plt.title('KNN Visualization using PCA')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# query_index = np.random.choice(combined_df_pivot.shape[0])\n",
        "\n",
        "recommend_knn(combined_df_matrix, combined_df_pivot, query_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FgqfK8rNlCGb"
      },
      "source": [
        "### Hybrid recommendation (Content Based + Collaborative )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5c-3pqXdktjv"
      },
      "outputs": [],
      "source": [
        "def hybrid_recommendation(user_id, item_id, rating_matrix, corr_matrix, combined_df_matrix, combined_df_pivot, knn):\n",
        "    # Content-based recommendation\n",
        "    content_based_recommendations = get_food_recipe_cos_content_based(item_id)\n",
        "\n",
        "    # Collaborative filtering recommendation\n",
        "    # Find item index corresponding to item_id\n",
        "    # item_index = combined_df_pivot.index.get_loc(item_id)  # Get the index of the item\n",
        "    n_recomm = 5\n",
        "    # Pass item_index to recommend_knn instead of user_id\n",
        "    # user_based_recommendations = recommend_knn(combined_df_matrix, combined_df_pivot, item_index)\n",
        "    item_based_recommendations = recommend_item_pearson(user_id, rating_matrix, corr_matrix, n_recomm)\n",
        "\n",
        "    # Combine recommendations\n",
        "    combined_recommendations = list(set(content_based_recommendations['Name'].tolist() + item_based_recommendations['Name'].tolist()))\n",
        "    # print(combined_recommendations)\n",
        "    # + user_based_recommendations['Name'].tolist()\n",
        "    # Return top n recommendations\n",
        "    top_n = 15\n",
        "    return combined_recommendations[:top_n]\n",
        "\n",
        "user_id = 1535\n",
        "item_id = 4\n",
        "recommendations = hybrid_recommendation(user_id, item_id, rating_matrix, corr_matrix, combined_df_matrix, combined_df_pivot, knn)\n",
        "\n",
        "# Print the recommendations outside the function\n",
        "print(f\"Hybrid recommendations for user {user_id} and item {item_id}:\")\n",
        "for i, recommendation in enumerate(recommendations, start=1):\n",
        "    print(f\"{i}. {recommendation}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZurRmmy2u3zV"
      },
      "source": [
        "### Graph neural network\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H70u6O4BtAwg"
      },
      "outputs": [],
      "source": [
        "del reviews\n",
        "del recipes_cp\n",
        "del recipes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CUi0bAeghHFB"
      },
      "outputs": [],
      "source": [
        "# A User can review multiple time\n",
        "Merged_df2[Merged_df2['RecipeId'] == 56]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVKYwt5slpGI"
      },
      "outputs": [],
      "source": [
        "# i would like to stratify the data to the recent details between 2010 to 2020\n",
        "\n",
        "# Merged_df2['ReviewDate'] = pd.to_datetime(Merged_df2['DatePublished'])\n",
        "Merged_df2['DatePublished'] = pd.to_datetime(Merged_df2['DatePublished'])\n",
        "Merged_df2['AuthorId'] = Merged_df2['AuthorId'].astype('int64')\n",
        "# Selecting of recipes published between 2015 and 2020\n",
        "filtered_df = Merged_df2[(Merged_df2['DatePublished'] >= '2014-01-01') & (Merged_df2['DatePublished'] <= '2020-12-31')]\n",
        "filtered_df.info()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_df['Rating'].value_counts()"
      ],
      "metadata": {
        "id": "uEWSBhoe8S2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MB5k8G4pzIja"
      },
      "outputs": [],
      "source": [
        "rec = filtered_df[['RecipeId', 'Name', 'RecipeCategory', 'Description', 'Calories', 'AuthorId'  ]]\n",
        "rev = filtered_df[['AuthorId', 'RecipeId', 'ReviewId', 'Rating']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3KGv1Uh6zhwa"
      },
      "outputs": [],
      "source": [
        "rating_df = rev\n",
        "recipe_fil_df = rec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0gU1ZcdjlWs"
      },
      "outputs": [],
      "source": [
        "# let's select the rows that has the same recipe ID as the recipe_df\n",
        "\n",
        "# merged_temp = recipe_df.merge(reviews, on='RecipeId', how='left')\n",
        "# merged_temp.sort_values(by='RecipeId')\n",
        "# rating_df = merged_temp[['AuthorId', 'RecipeId', 'Rating']]\n",
        "# rating_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgaXBaEAojK8"
      },
      "outputs": [],
      "source": [
        "subset_rating = (\n",
        "    review_df.groupby('AuthorId')['Rating']\n",
        "    .agg(['count'])\n",
        "    .rename(columns={'count': 'AutCount'})\n",
        "    .sort_values(by='AutCount', ascending=False)\n",
        "    .reset_index()  # This line ensures the RecipeId is part of the DataFrame\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5aXctyLYx3CS"
      },
      "outputs": [],
      "source": [
        "rating_df.nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjgF-YSgu_DV"
      },
      "outputs": [],
      "source": [
        "categories = []\n",
        "for item in recipe_fil_df['RecipeCategory']:\n",
        "  if item not in categories:\n",
        "    categories.append(item)\n",
        "  # categories.append(item.split(','))\n",
        "print(categories)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IYz3Fkj1gbn"
      },
      "outputs": [],
      "source": [
        "len(categories)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "td0ulmdXS_ap"
      },
      "outputs": [],
      "source": [
        "categories = list(set(categories))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# rating_df.groupby('RecipeCategory').agg('count')[5].plot(kind='bar')\n",
        "\n",
        "category_counts = recipe_fil_df.groupby('RecipeCategory').size().sort_values(ascending=False)\n",
        "\n",
        "# Plot the top 20 as a bar chart\n",
        "category_counts.head(20).plot(kind='bar', figsize=(10, 6))\n",
        "\n",
        "plt.xlabel('Recipe Category')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Count of Recipes by Category')\n",
        "plt.xticks(rotation=75)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WhEb7pEPwl5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojFllem02-02"
      },
      "outputs": [],
      "source": [
        "label_encoder = LabelEncoder()\n",
        "# onehot_encoder = OneHotEncoder(sparse_output=False)\n",
        "label_enc_cat = label_encoder.fit_transform(categories)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BJWY6YkS9gvB"
      },
      "outputs": [],
      "source": [
        "recipe_fil_df['RecipeCategoryEnc'] = label_encoder.transform(recipe_fil_df['RecipeCategory'])\n",
        "recipe_fil_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7G2aZwg9nz_"
      },
      "outputs": [],
      "source": [
        "enc_shape = label_enc_cat.reshape(len(label_enc_cat), 1)\n",
        "enc_shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TOqHyHW24wTc"
      },
      "outputs": [],
      "source": [
        "ohe = OneHotEncoder(sparse_output=False)\n",
        "ohe_encode = ohe.fit_transform(enc_shape)\n",
        "# ohe.set_output(transform=\"default\")\n",
        "np.array(ohe_encode)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hu2igKyBCTNn"
      },
      "outputs": [],
      "source": [
        "categories\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oadNKS0V7K8J"
      },
      "outputs": [],
      "source": [
        "def vector_to_cat(vector) -> str:\n",
        "  return label_encoder.inverse_transform([np.argmax(vector)])[0]\n",
        "\n",
        "# convert categoories to one hot feature vector\n",
        "def cat_to_vector(category):\n",
        "  # categories = list(set(category))\n",
        "  # print(categories)\n",
        "  int_encoded = label_encoder.transform([category])\n",
        "  int_encoded = int_encoded.reshape(-1, 1)\n",
        "  # print(int_encoded)\n",
        "  # onehot_encoder = OneHotEncoder(sparse=False)\n",
        "  onehot_encoded = ohe.transform(int_encoded)\n",
        "  onehot_encoded = np.array(onehot_encoded)\n",
        "  return onehot_encoded[0]\n",
        "  # output = onehot_encoded[0] # Initialize with the first row\n",
        "  # # print(output_arr)\n",
        "  # for i in range(1, len(onehot_encoded)):\n",
        "  #   # output = np.concatenate((output, onehot_encoded[i]))\n",
        "  #   output += onehot_encoded[i]\n",
        "  # return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyueqbazKlGS"
      },
      "outputs": [],
      "source": [
        "class TextEncoder:\n",
        "  def __init__(self, model='all-MiniLM-L6-v2', device=None):\n",
        "    self.device = device\n",
        "    self.model = SentenceTransformer(model, device=self.device)\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def __call__(self, value: list):\n",
        "\n",
        "    x = self.model.encode(value, show_progress_bar=True, convert_to_tensor=True, device=self.device)\n",
        "    return x.cpu()\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Device: {device}')\n",
        "\n",
        "encoder = TextEncoder(device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLiRUwl47Mzs"
      },
      "outputs": [],
      "source": [
        "rating = rating_df[['AuthorId', 'RecipeId', 'Rating']].reset_index(drop=True)\n",
        "rating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xpB3VywaDYl-"
      },
      "outputs": [],
      "source": [
        "# recipe_df = recipe_df[['RecipeId', 'Name', 'RecipeCategory', 'Description', 'Keywords', 'Calories'  ]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8H2qa8Y4k2e"
      },
      "outputs": [],
      "source": [
        "# Let's create a dataframe that includes the unique userId,\n",
        "#  this will be used to create nodes in the graph\n",
        "\n",
        "unique_user_id = rating['AuthorId'].unique()\n",
        "unique_user_id = pd.DataFrame(data={\n",
        "    'userId': unique_user_id,\n",
        "    'mappedId': pd.RangeIndex(len(unique_user_id))\n",
        "})\n",
        "\n",
        "unique_user_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UWLRN9my5nU1"
      },
      "outputs": [],
      "source": [
        "unique_recipe_id = recipe_fil_df['RecipeId'].unique()\n",
        "unique_recipe_id = pd.DataFrame(data={\n",
        "    'recipeId': unique_recipe_id,\n",
        "    'mappedId': pd.RangeIndex(len(unique_recipe_id))\n",
        "})\n",
        "\n",
        "unique_recipe_id = unique_recipe_id.merge(recipe_fil_df[['RecipeId', 'RecipeCategoryEnc']], left_on='recipeId',right_on='RecipeId', how='left')\n",
        "unique_recipe_id.drop('RecipeId', axis=1, inplace=True)\n",
        "unique_recipe_id.drop_duplicates(inplace=True)\n",
        "# unique_recipe_id.sort_values(by='mappedId', inplace=True)\n",
        "unique_recipe_id.reset_index(drop=True, inplace=True)\n",
        "unique_recipe_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQe8h8EO6t-k"
      },
      "outputs": [],
      "source": [
        "print(f'The number of unique users: {unique_user_id.shape[0]}')\n",
        "print(f'The number of unique recipes: {unique_recipe_id.shape[0]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPo0FoqwEIdi"
      },
      "outputs": [],
      "source": [
        "# creating a vector for each of the category\n",
        "recipe_fil_df['RecipeCategoryvector'] = recipe_fil_df['RecipeCategory'].apply(cat_to_vector)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4-4hPP9Mol6"
      },
      "outputs": [],
      "source": [
        "len(recipe_fil_df['RecipeCategoryvector'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9jL2VaBcQxQ"
      },
      "outputs": [],
      "source": [
        "# creating a recipe feature torch.tensor\n",
        "\n",
        "recipe_features = torch.zeros(len(unique_recipe_id), len(ohe_encode))\n",
        "recipe_features.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYcKxsQGcdkA"
      },
      "outputs": [],
      "source": [
        "recipe_fil_df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvCPT1_IRws8"
      },
      "outputs": [],
      "source": [
        "# creating a recipe feature torch.tensor\n",
        "\n",
        "recipe_features = torch.zeros(len(unique_recipe_id), 384 + len(ohe_encode))\n",
        "# recipe_features\n",
        "def convert_to_embed(x):\n",
        "  recipe = x['RecipeId']\n",
        "  # getting the mapped Id\n",
        "  mapped_id = unique_recipe_id.loc[unique_recipe_id['recipeId'] == recipe, 'mappedId'].values[0]\n",
        "  # getting the vector and converting it to tensor\n",
        "  one_hot = torch.tensor(x['RecipeCategoryvector'])\n",
        "  # desc = ' '.join(x['Description'])\n",
        "  # print(x['Description'])\n",
        "  # checking if the description is empty\n",
        "  if x['Description'] == '':\n",
        "    recipe_features[mapped_id] = torch.cat((encoder('None'), one_hot), -1)\n",
        "  else:\n",
        "    recipe_features[mapped_id] = torch.cat((encoder(x['Description']), one_hot), -1)\n",
        "# print(recipe_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "wyUVS0vTu4pk"
      },
      "outputs": [],
      "source": [
        "recipe_fil_df.apply(lambda x: convert_to_embed(x), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8HZMtVDGI_d"
      },
      "outputs": [],
      "source": [
        "recipe_features[1233]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7sVGurwGgh9"
      },
      "outputs": [],
      "source": [
        "# i will try creating edges for user-user as well\n",
        "#  here, i will get a user, and also get a list of other users that rate the same product"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMJj33FKKZyU"
      },
      "outputs": [],
      "source": [
        "# creating user-recipe edge tensor\n",
        "\n",
        "user_recipe_edges = []\n",
        "user_recipe_ratings = []\n",
        "for rating_info in rating.itertuples():\n",
        "  # print(rating_info.RecipeId)\n",
        "  user_recipe_edges.append([unique_user_id.loc[unique_user_id['userId'] == rating_info.AuthorId, 'mappedId'].values[0],\n",
        "                           unique_recipe_id.loc[unique_recipe_id['recipeId'] == rating_info.RecipeId, 'mappedId'].values[0]])\n",
        "  user_recipe_ratings.append(rating_info.Rating)\n",
        "\n",
        "user_recipe_edges = torch.t(torch.tensor(user_recipe_edges, dtype=torch.long))\n",
        "user_recipe_ratings = torch.tensor(user_recipe_ratings, dtype=torch.float)\n",
        "\n",
        "print(user_recipe_edges.shape)\n",
        "print(user_recipe_ratings.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_recipe_id.sort_values(by='RecipeCategoryEnc', inplace=True)\n",
        "# unique_recipe_id.reset_index(drop=True, inplace=True)\n",
        "unique_recipe_id"
      ],
      "metadata": {
        "id": "cd5xgFFpmLGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "# Create a mapping from RecipeCategoryEnc to a list of mappedIds\n",
        "category_to_recipes = defaultdict(list)\n",
        "for _, row in unique_recipe_id.iterrows():\n",
        "    category_to_recipes[row['RecipeCategoryEnc']].append(row['mappedId'])\n",
        "\n",
        "# Generate edges\n",
        "recipe_recipe_edges = []\n",
        "for recipes in category_to_recipes.values():\n",
        "    for i in range(len(recipes)):\n",
        "        for j in range(i + 1, len(recipes)):\n",
        "            recipe_recipe_edges.append([recipes[i], recipes[j]])\n",
        "\n",
        "# Convert to tensor\n",
        "recipe_recipe_edges = torch.t(torch.tensor(recipe_recipe_edges, dtype=torch.long))\n",
        "# print(recipe_recipe_edges.nunique())\n",
        "recipe_recipe_edges.shape\n"
      ],
      "metadata": {
        "id": "CyPgLMq4vxKu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5Tw9T5bO0W0"
      },
      "outputs": [],
      "source": [
        "# recipe_recipe_edge = []\n",
        "# for recipe_info in recipe_df.itertuples():\n",
        "#   recipe_recipe_edge.append([ unique_recipe_id.loc[unique_recipe_id['recipeId'] == recipe_info.RecipeId, 'mappedId'].values[0],\n",
        "#                              unique_recipe_id.loc[unique_recipe_id['RecipeCategoryEnc'] == recipe_info.RecipeCategoryEnc, 'mappedId'].values[0]])\n",
        "# recipe_recipe_edge = torch.t(torch.tensor(recipe_recipe_edge, dtype=torch.long))\n",
        "# recipe_recipe_edge.shape\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJ9Vg4Ytssji"
      },
      "outputs": [],
      "source": [
        "recipe_features.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vOlnEnBMUkZO"
      },
      "outputs": [],
      "source": [
        "# creating a heterogenuous graph\n",
        "from  torch_geometric.data import HeteroData\n",
        "import torch_geometric.transforms as T\n",
        "\n",
        "data = HeteroData()\n",
        "\n",
        "data[\"User\"].node_id = torch.arange(len(unique_user_id))\n",
        "data[\"Recipe\"].node_id = torch.arange(len(unique_recipe_id))\n",
        "# Adding node features to the HeteroData object\n",
        "data['Recipe'].x = recipe_features\n",
        "data['User'].x = torch.eye(len(recipe_fil_df['AuthorId'].unique()))\n",
        "\n",
        "# Adding edges indices to the HeteroData object\n",
        "data['User', 'RATING', 'Recipe'].edge_index = user_recipe_edges\n",
        "data['User', 'RATING', 'Recipe'].edge_label = user_recipe_ratings\n",
        "data['Recipe', 'CATEGORY', 'Recipe'].edge_index= recipe_recipe_edges\n",
        "\n",
        "data = T.ToUndirected()(data)\n",
        "\n",
        "del data['Recipe', 'rev_RATING', 'User'].edge_label\n",
        "# del data['Recipe', 'rev_CATEGORY', 'Recipe'].edge_label\n",
        "\n",
        "data\n",
        "\n",
        "# the increament in the edge index of [2, 61324] because of the reverse egge created"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hsfcyrj2x6yw"
      },
      "outputs": [],
      "source": [
        "data['Recipe', 'CATEGORY', 'Recipe'].edge_index.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q_lCjKdrqh8c"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.transforms import RandomLinkSplit\n",
        "\n",
        "# since there is no float in rating i.e, the rating ranges for 1 to 5,\n",
        "data['User', 'RATING', 'Recipe'].edge_label = data['User', 'RATING', 'Recipe'].edge_label.long()\n",
        "\n",
        "# Apply RandomLinkSplit, specifying the number of nodes for each node type\n",
        "transform = T.RandomLinkSplit(\n",
        "    num_val=0.2,\n",
        "    num_test=0.1,\n",
        "    disjoint_train_ratio=0.3,\n",
        "    neg_sampling_ratio=0.0,\n",
        "    add_negative_train_samples=False,\n",
        "    edge_types=[('User', 'RATING', 'Recipe'), ('Recipe', 'CATEGORY', 'Recipe')],\n",
        "    rev_edge_types=[('Recipe', 'rev_RATING', 'User'), ('Recipe', 'rev_CATEGORY', 'Recipe')]\n",
        ")\n",
        "train_data, val_data, test_data = transform(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9W5fk7sL5PcG"
      },
      "outputs": [],
      "source": [
        "train_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yv2sQ2zIEfLM"
      },
      "outputs": [],
      "source": [
        "G = nx.Graph()\n",
        "\n",
        "# Add edges from each edge type\n",
        "for edge_type in data.edge_types:\n",
        "    if edge_type != ('Recipe', 'rev_RATING', 'User'):  # Skip the missing edge\n",
        "        src, dst = data[edge_type].edge_index\n",
        "        for i in range(src.size(0)):\n",
        "            G.add_edge(src[i].item(), dst[i].item())\n",
        "\n",
        "# Plotting the graph\n",
        "plt.figure(figsize=(12, 8))  # Adjust the figure size as needed\n",
        "\n",
        "# Draw nodes with default options\n",
        "nx.draw(G, with_labels=True, node_size=300, font_size=10, node_color='skyblue', edge_color='gray', linewidths=0.5, font_color='black')\n",
        "\n",
        "plt.title('Heterogeneous Graph User_Recipe')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNELNup_Bi0O"
      },
      "source": [
        "#### Model Building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09_anMV8-U_d"
      },
      "outputs": [],
      "source": [
        "# ENCODER\n",
        "# hidden channel is the dimentionality learned for each node\n",
        "class GNNEncoder(torch.nn.Module):\n",
        "  def __init__(self, hidden_channels, out_channels):\n",
        "    super().__init__()\n",
        "    self.conv1 = SAGEConv((-1, -1), hidden_channels)\n",
        "    self.conv2 = SAGEConv((-1, -1), out_channels)\n",
        "\n",
        "  def forward(self, x, edge_index):\n",
        "    x = self.conv1(x, edge_index).relu()\n",
        "    x = self.conv2(x, edge_index)\n",
        "    return x\n",
        "\n",
        "#DECODER\n",
        "class EdgeDecoder(torch.nn.Module):\n",
        "  def __init__(self, hidden_channels):\n",
        "    super().__init__()\n",
        "    self.lin1 = torch.nn.Linear(2 * hidden_channels, hidden_channels)\n",
        "    self.lin2 = torch.nn.Linear(hidden_channels, 1)\n",
        "\n",
        "  def forward(self, z_dict, edge_label_index):\n",
        "    row, col = edge_label_index\n",
        "    z = torch.cat([z_dict['User'][row], z_dict['Recipe'][col]], dim=-1)\n",
        "\n",
        "    z = self.lin1(z).relu()\n",
        "    z = self.lin2(z)\n",
        "    return z.view(-1)\n",
        "\n",
        "# ENCODER-DECODER MODEL\n",
        "class GNNModel(torch.nn.Module):\n",
        "  def __init__(self, hidden_channels):\n",
        "    super().__init__()\n",
        "    self.encoder = GNNEncoder(hidden_channels, hidden_channels)\n",
        "    self.encoder = to_hetero(self.encoder, data.metadata(), aggr='mean')\n",
        "    self.decoder = EdgeDecoder(hidden_channels)\n",
        "\n",
        "  def forward(self, x_dict, edge_index_dict, edge_label_index):\n",
        "    edge_index_dict = {k: v.to(torch.long).view(2, -1) for k, v in edge_index_dict.items()}\n",
        "    z_dict = self.encoder(x_dict, edge_index_dict)\n",
        "    return self.decoder(z_dict, edge_label_index)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = GNNModel(hidden_channels=32).to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8Vf6gtGmT3F"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GATConv, to_hetero\n",
        "\n",
        "# ENCODER\n",
        "# hidden_channels is the dimensionality learned for each node\n",
        "class GNNEncoder(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = SAGEConv((-1, -1), hidden_channels//4)\n",
        "        self.conv2 = SAGEConv((-1, -1), hidden_channels//2)\n",
        "        self.conv3 = SAGEConv((-1, -1), out_channels)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        x = self.conv2(x, edge_index).relu()\n",
        "        x = self.conv3(x, edge_index)\n",
        "        return x\n",
        "\n",
        "# DECODER\n",
        "class EdgeDecoder(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super().__init__()\n",
        "        self.lin1 = torch.nn.Linear(2 * hidden_channels, hidden_channels)\n",
        "        self.lin2 = torch.nn.Linear(hidden_channels, hidden_channels // 2)\n",
        "        self.lin3 = torch.nn.Linear(hidden_channels // 2, 1)\n",
        "\n",
        "    def forward(self, z_dict, edge_label_index):\n",
        "        row, col = edge_label_index\n",
        "        z = torch.cat([z_dict['User'][row], z_dict['Recipe'][col]], dim=-1)\n",
        "        z = self.lin1(z).relu()\n",
        "        z = self.lin2(z).relu()\n",
        "        z = self.lin3(z)\n",
        "        return z.view(-1)\n",
        "\n",
        "# ENCODER-DECODER MODEL\n",
        "class GNNModel(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super().__init__()\n",
        "        self.encoder = GNNEncoder(hidden_channels, hidden_channels)\n",
        "        self.encoder = to_hetero(self.encoder, data.metadata(), aggr='mean')\n",
        "        self.decoder = EdgeDecoder(hidden_channels)\n",
        "\n",
        "    def forward(self, x_dict, edge_index_dict, edge_label_index):\n",
        "        edge_index_dict = {k: v.to(torch.long).view(2, -1) for k, v in edge_index_dict.items()}\n",
        "        z_dict = self.encoder(x_dict, edge_index_dict)\n",
        "        return self.decoder(z_dict, edge_label_index)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = GNNModel(hidden_channels=32).to(device)\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYJDbG2NHxbm"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import torch.nn.functional as F\n",
        "# import torch.optim as optim\n",
        "\n",
        "# Define the optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "# optimizer = torch.optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
        "\n",
        "# Define the training function\n",
        "def train():\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    pred = model(train_data.x_dict, train_data.edge_index_dict,\n",
        "                 train_data['User','Recipe'].edge_label_index)\n",
        "    target = train_data['User', 'Recipe'].edge_label.float()\n",
        "    loss = F.mse_loss(pred, target).sqrt()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return float(loss)\n",
        "\n",
        "# Define the test function\n",
        "@torch.no_grad()\n",
        "def test(data):\n",
        "    data = data.to(device)\n",
        "    model.eval()\n",
        "    pred = model(data.x_dict, data.edge_index_dict,\n",
        "                 data['User', 'Recipe'].edge_label_index)\n",
        "    pred = pred.clamp(min=0, max=5)\n",
        "    target = data['User', 'Recipe'].edge_label.float()\n",
        "    rmse = F.mse_loss(pred, target).sqrt()\n",
        "    return float(rmse)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 1000\n",
        "train_loss = []\n",
        "valid_loss = []\n",
        "best_val_rmse = float('inf')\n",
        "best_epoch = -1\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train_data = train_data.to(device)\n",
        "    loss = train()\n",
        "    train_rmse = test(train_data)\n",
        "    val_rmse = test(val_data)\n",
        "    train_loss.append(train_rmse)\n",
        "    valid_loss.append(val_rmse)\n",
        "\n",
        "    if val_rmse < best_val_rmse:\n",
        "        best_val_rmse = val_rmse\n",
        "        best_epoch = epoch\n",
        "        # Save the best model\n",
        "        torch.save(model.state_dict(), 'SAGE_best_model.pth')\n",
        "\n",
        "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train_RMSE: {train_rmse:.4f}, '\n",
        "          f'Val_RMSE: {val_rmse:.4f}')\n",
        "          #Best_Val_RMSE: {best_val_rmse:.4f} at Epoch: {best_epoch:03d}'\n",
        "\n",
        "print(f'Best Validation RMSE: {best_val_rmse:.4f} at Epoch: {best_epoch}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RqB3-gLbI8pr"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(train_loss)\n",
        "plt.plot(valid_loss)\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Load the best model's state_dict\n",
        "model.load_state_dict(torch.load('SAGE_best_model.pth'))\n",
        "# Move model to the appropriate device\n",
        "model.to(device)\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "# Move test data to device\n",
        "test_data = test_data.to(device)\n",
        "\n",
        "# Make predictions\n",
        "with torch.no_grad():\n",
        "    pred = model(test_data.x_dict, test_data.edge_index_dict,\n",
        "                 test_data['User', 'Recipe'].edge_label_index)\n",
        "    pred = pred.clamp(min=0, max=5)  # Ensure predictions are within valid rating range\n",
        "    target = test_data['User', 'Recipe'].edge_label.float()\n",
        "    rmse = F.mse_loss(pred, target).sqrt()\n",
        "    print(f'Test RMSE: {rmse:.4f}')\n",
        "\n",
        "    # Extract userId and recipeId\n",
        "    userId = test_data['User', 'Recipe'].edge_label_index[0].cpu().numpy()\n",
        "    recipeId = test_data['User', 'Recipe'].edge_label_index[1].cpu().numpy()\n",
        "\n",
        "    # Convert predictions and targets to numpy\n",
        "    pred = pred.cpu().numpy()\n",
        "    target = target.cpu().numpy()\n",
        "\n",
        "    # Create a DataFrame with the results\n",
        "    result = pd.DataFrame({'UserId': userId, 'RecipeId': recipeId, 'PredictedRating': pred, 'ActualRating': target})\n",
        "\n",
        "result\n"
      ],
      "metadata": {
        "id": "EFrEVk1ThaBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fQRivJHtTAqy"
      },
      "outputs": [],
      "source": [
        "# with torch.no_grad():\n",
        "#   test_data = test_data.to(device)\n",
        "#   pred = model(test_data.x_dict, test_data.edge_index_dict,\n",
        "#                test_data['User', 'Recipe'].edge_label_index)\n",
        "#   pred = pred.clamp(min=0, max=5)\n",
        "#   target = test_data['User', 'Recipe'].edge_label.float()\n",
        "#   rmse = F.mse_loss(pred, target).sqrt()\n",
        "#   print(f'Test RMSE: {rmse:.4f}')\n",
        "\n",
        "# userId = test_data['User', 'Recipe'].edge_label_index[0].cpu().numpy()\n",
        "# recipeId = test_data['User', 'Recipe'].edge_label_index[1].cpu().numpy()\n",
        "\n",
        "# pred = pred.cpu().numpy()\n",
        "# target = target.cpu().numpy()\n",
        "\n",
        "# result = pd.DataFrame({'UserId': userId, 'RecipeId': recipeId, 'PredictedRating': pred, 'ActualRating': target})\n",
        "# result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnJtKE38BSf_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJ7XFOelBS_P"
      },
      "source": [
        "GAT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eucqbgjZfzrU"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import torch.nn.functional as F\n",
        "# from torch_geometric.nn import GATConv, to_hetero\n",
        "\n",
        "# ENCODER\n",
        "# hidden_channels is the dimensionality learned for each node\n",
        "class GNNEncoder(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.conv1 = GATConv((-1, -1), hidden_channels//4, heads=2, add_self_loops=False)\n",
        "        self.conv2 = GATConv((-1, -1), hidden_channels//2, heads=2, add_self_loops=False)\n",
        "        self.conv3 = GATConv((-1, -1), out_channels, heads=1, add_self_loops=False)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index).relu()\n",
        "        x = self.conv2(x, edge_index).relu()\n",
        "        x = self.conv3(x, edge_index)\n",
        "        return x\n",
        "\n",
        "# DECODER\n",
        "class EdgeDecoder(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super().__init__()\n",
        "        self.lin1 = torch.nn.Linear(2 * hidden_channels, hidden_channels)\n",
        "        self.lin2 = torch.nn.Linear(hidden_channels, hidden_channels // 2)\n",
        "        self.lin3 = torch.nn.Linear(hidden_channels//2, hidden_channels // 4)\n",
        "        self.lin4 = torch.nn.Linear(hidden_channels // 4, 1)\n",
        "\n",
        "    def forward(self, z_dict, edge_label_index):\n",
        "        row, col = edge_label_index\n",
        "        z = torch.cat([z_dict['User'][row], z_dict['Recipe'][col]], dim=-1)\n",
        "        z = self.lin1(z).relu()\n",
        "        z = self.lin2(z).relu()\n",
        "        z = self.lin3(z).relu()\n",
        "        z = self.lin4(z)\n",
        "        return z.view(-1)\n",
        "\n",
        "# ENCODER-DECODER MODEL\n",
        "class GNNModel(torch.nn.Module):\n",
        "    def __init__(self, hidden_channels):\n",
        "        super().__init__()\n",
        "        self.encoder = GNNEncoder(hidden_channels, hidden_channels)\n",
        "        self.encoder = to_hetero(self.encoder, data.metadata(), aggr='mean')\n",
        "        self.decoder = EdgeDecoder(hidden_channels)\n",
        "\n",
        "    def forward(self, x_dict, edge_index_dict, edge_label_index):\n",
        "        edge_index_dict = {k: v.to(torch.long).view(2, -1) for k, v in edge_index_dict.items()}\n",
        "        z_dict = self.encoder(x_dict, edge_index_dict)\n",
        "        return self.decoder(z_dict, edge_label_index)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "GATmodel = GNNModel(hidden_channels=32).to(device)\n",
        "print(GATmodel)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBBpkOzZBlLk"
      },
      "outputs": [],
      "source": [
        "# Define the optimizer\n",
        "optimizer = torch.optim.Adam(GATmodel.parameters(), lr=0.0001)\n",
        "# optimizer = torch.optim.SGD(GATmodel.parameters(), lr=0.0001, momentum=0.9)\n",
        "\n",
        "\n",
        "# Define the training function\n",
        "def train():\n",
        "    GATmodel.train()\n",
        "    optimizer.zero_grad()\n",
        "    pred = GATmodel(train_data.x_dict, train_data.edge_index_dict,\n",
        "                 train_data['User', 'Recipe'].edge_label_index)\n",
        "    target = train_data['User', 'Recipe'].edge_label.float()\n",
        "    loss = F.mse_loss(pred, target).sqrt()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    return float(loss)\n",
        "\n",
        "# Define the test function\n",
        "@torch.no_grad()\n",
        "def test(data):\n",
        "    data = data.to(device)\n",
        "    GATmodel.eval()\n",
        "    pred = GATmodel(data.x_dict, data.edge_index_dict,\n",
        "                 data['User', 'Recipe'].edge_label_index)\n",
        "    pred = pred.clamp(min=0, max=5)\n",
        "    target = data['User', 'Recipe'].edge_label.float()\n",
        "    rmse = F.mse_loss(pred, target).sqrt()\n",
        "    return float(rmse)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 1500\n",
        "train_loss = []\n",
        "valid_loss = []\n",
        "best_val_rmse = float('inf')\n",
        "best_epoch = -1\n",
        "\n",
        "for epoch in range(1, num_epochs + 1):\n",
        "    train_data = train_data.to(device)\n",
        "    loss = train()\n",
        "    train_rmse = test(train_data)\n",
        "    val_rmse = test(val_data)\n",
        "    train_loss.append(train_rmse)\n",
        "    valid_loss.append(val_rmse)\n",
        "\n",
        "    if val_rmse < best_val_rmse:\n",
        "        best_val_rmse = val_rmse\n",
        "        best_epoch = epoch\n",
        "        # Save the best model\n",
        "        torch.save(GATmodel.state_dict(), 'GAT_best_model.pth')\n",
        "\n",
        "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train_RMSE: {train_rmse:.4f}, '\n",
        "          f'Val_RMSE: {val_rmse:.4f}')\n",
        "          #Best_Val_RMSE: {best_val_rmse:.4f} at Epoch: {best_epoch:03d}'\n",
        "\n",
        "print(f'Best Validation RMSE: {best_val_rmse:.4f} at Epoch: {best_epoch}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XUgyEgLFBuhe"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(train_loss)\n",
        "plt.plot(valid_loss)\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GATmodel = GNNModel(hidden_channels=64).to(device)\n",
        "# Load the best model's state_dict\n",
        "GATmodel.load_state_dict(torch.load('GAT_best_model.pth'))\n",
        "# Move model to the appropriate device\n",
        "GATmodel.to(device)\n",
        "# Set model to evaluation mode\n",
        "GATmodel.eval()\n",
        "# Move test data to device\n",
        "test_data = test_data.to(device)\n",
        "\n",
        "# Make predictions\n",
        "with torch.no_grad():\n",
        "    pred = GATmodel(test_data.x_dict, test_data.edge_index_dict,\n",
        "                 test_data['User', 'Recipe'].edge_label_index)\n",
        "    pred = pred.clamp(min=0, max=5)  # Ensure predictions are within valid rating range\n",
        "    target = test_data['User', 'Recipe'].edge_label.float()\n",
        "    rmse = F.mse_loss(pred, target).sqrt()\n",
        "    print(f'Test RMSE: {rmse:.4f}')\n",
        "\n",
        "    # Extract userId and recipeId\n",
        "    userId = test_data['User', 'Recipe'].edge_label_index[0].cpu().numpy()\n",
        "    recipeId = test_data['User', 'Recipe'].edge_label_index[1].cpu().numpy()\n",
        "\n",
        "    # Convert predictions and targets to numpy\n",
        "    pred = pred.cpu().numpy()\n",
        "    target = target.cpu().numpy()\n",
        "\n",
        "    # Create a DataFrame with the results\n",
        "    result = pd.DataFrame({'UserId': userId, 'RecipeId': recipeId, 'PredictedRating': pred, 'ActualRating': target})\n",
        "\n",
        "result\n"
      ],
      "metadata": {
        "id": "MCOInMaWmgAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXIeBrFuCXpc"
      },
      "outputs": [],
      "source": [
        "max_user_review = rating_df.groupby('AuthorId')['ReviewId'].count().sort_values(ascending=False)\n",
        "# max_recipe_review = rating_df.groupby('RecipeId').size().max()\n",
        "max_user_review"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoT7vVxQRvTt"
      },
      "outputs": [],
      "source": [
        "\n",
        "max_user_id = max_user_review.idxmax()\n",
        "max_user_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEYmxu3JUZVm"
      },
      "outputs": [],
      "source": [
        "#let's rate a product noot rated by the user\n",
        "mapped_user_id = unique_user_id.loc[unique_user_id['userId']==max_user_id, 'mappedId'].values[0]\n",
        "print('the mapped id of user', max_user_id, 'who has the maximum review is' , mapped_user_id)\n",
        "recipe_rated = rating_df[rating_df['AuthorId']==max_user_id]\n",
        "recipe_unrated = recipe_fil_df[~recipe_fil_df['RecipeId'].isin(recipe_rated['RecipeId'])]\n",
        "recipe_unrated = recipe_unrated.merge(unique_recipe_id, left_on='RecipeId', right_on='recipeId', how='inner')\n",
        "print('--------The sampled recipe that ha not been rated by the user-------------------')\n",
        "recipe_item = recipe_unrated.sample(1)\n",
        "recipe_item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGcxvP5TYTw2"
      },
      "outputs": [],
      "source": [
        "\n",
        "recipe_item = recipe_item['mappedId'].item()\n",
        "recipe_item"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjnVqTtgdo5m"
      },
      "outputs": [],
      "source": [
        "# considering the product hasn't be rated, let us rate the recipe\n",
        "# the tenspr is the id of the max reviewed rated item with one of the unrated item, let's predict\n",
        "# what the user will predict for the product, this will solve the cold start problem\n",
        "edge_label_index = torch.tensor([mapped_user_id, recipe_item], dtype=torch.long)\n",
        "edge_label_index\n",
        "with torch.no_grad():\n",
        "  test_data.to(device)\n",
        "  pred = model(test_data.x_dict, test_data.edge_index_dict, edge_label_index)\n",
        "  pred = pred.clamp(min=0, max=5).detach().cpu().numpy()\n",
        "print(pred.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xp5GSqg0eTGF"
      },
      "outputs": [],
      "source": [
        "explainer = Explainer(\n",
        "    model = model,\n",
        "    algorithm=CaptumExplainer('IntegratedGradients'),\n",
        "    explanation_type='model',\n",
        "    model_config = dict(\n",
        "        mode='regression',\n",
        "        task_level='edge',\n",
        "        return_type='raw'\n",
        "    ),\n",
        "    node_mask_type=None,\n",
        "    edge_mask_type='object'\n",
        ")\n",
        "\n",
        "explanation = explainer(\n",
        "    test_data.x_dict,\n",
        "    test_data.edge_index_dict,\n",
        "    index=0,\n",
        "    edge_label_index=edge_label_index\n",
        ").cpu().detach()\n",
        "\n",
        "explanation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OzaTqqauq7nQ"
      },
      "outputs": [],
      "source": [
        "recipe_id = unique_recipe_id.loc[unique_recipe_id['mappedId']==recipe_item, 'recipeId'].values[0]\n",
        "recipe_title = recipe_fil_df.loc[recipe_fil_df['RecipeId']==recipe_id, 'Name'].values[0]\n",
        "# print(recipe_title)recipe_fil_df\n",
        "user_to_recipe = explanation['User', 'Recipe'].edge_index.numpy().T\n",
        "user_to_recipe_attr = explanation['User', 'Recipe'].edge_mask.numpy().T\n",
        "user_to_recipe_df = pd.DataFrame(\n",
        "    np.hstack([user_to_recipe, user_to_recipe_attr.reshape(-1, 1)]),\n",
        "    columns = ['mappedUserId', 'mappedRecipeId', 'attr']\n",
        ")\n",
        "\n",
        "recipe_to_user = explanation['Recipe', 'User'].edge_index.numpy().T\n",
        "recipe_to_user_attr = explanation['Recipe', 'User'].edge_mask.numpy().T\n",
        "recipe_to_user_df = pd.DataFrame(\n",
        "    np.hstack([recipe_to_user, recipe_to_user_attr.reshape(-1, 1)]),\n",
        "    columns = ['mappedRecipeId', 'mappedUserId', 'attr']\n",
        ")\n",
        "\n",
        "explanation_df = pd.concat([user_to_recipe_df, recipe_to_user_df])\n",
        "explanation_df[['mappedUserId', 'mappedRecipeId']] = explanation_df[['mappedUserId', 'mappedRecipeId']].astype(int)\n",
        "\n",
        "print(f\"Attribtion for all edges towards prediction of Recipe rating of Recipe: \\n {recipe_title}\")\n",
        "print(explanation_df.sort_values(by='attr'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IRp1d-4LtywL"
      },
      "outputs": [],
      "source": [
        "print(mapped_user_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDLn5LME8nMo"
      },
      "outputs": [],
      "source": [
        "unique_recipe_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46xTRzrBHWDL"
      },
      "outputs": [],
      "source": [
        "explanation_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8CEVHTVSlVx"
      },
      "outputs": [],
      "source": [
        "def recipe_recommendation_GNN(explanation_df, recipe_fil_df, unique_recipe_id, mapped_user_id):\n",
        "    explanation_df = explanation_df[explanation_df['mappedUserId'] == mapped_user_id]\n",
        "    explanation_df = explanation_df.groupby('mappedRecipeId').sum()\n",
        "    explanation_df = explanation_df.merge(unique_recipe_id, left_on='mappedRecipeId', right_on='mappedId', how='inner')\n",
        "    recipe_df_unique = recipe_fil_df.drop_duplicates(subset=['RecipeId'])\n",
        "    explanation_df = explanation_df.merge(recipe_df_unique, left_on='recipeId', right_on='RecipeId', how='inner')\n",
        "    pd.options.display.float_format = \"{:,.9f}\".format\n",
        "    # print(explanation_df.columns)\n",
        "\n",
        "    print(\"Top products that contributed to the prediction\")\n",
        "    exp = explanation_df.sort_values(by='attr', ascending=False, key=lambda x: abs(x))[['RecipeId', 'Name', 'Calories', 'RecipeCategory', 'attr']].head(5)\n",
        "\n",
        "    return exp\n",
        "\n",
        "recommendation = recipe_recommendation_GNN(explanation_df, recipe_fil_df, unique_recipe_id, mapped_user_id)\n",
        "recommendation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Hybrid Recommnedation Sytem (GNN + Content based)"
      ],
      "metadata": {
        "id": "QLSCjmODL2TW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def hybrid_recipe_recommendation(explanation_df, recipe_df, unique_recipe_id, mapped_user_id, cos_mat):\n",
        "#     gnn_recommendations = recipe_recommendation_GNN(explanation_df, recipe_df, unique_recipe_id, mapped_user_id)\n",
        "#     content_based_recommendations = get_food_recipe_cos_content_based(gnn_recommendations.index)\n",
        "\n",
        "#     # Assign weights to each system (adjust weights as needed)\n",
        "#     gnn_weight = 0.7\n",
        "#     content_based_weight = 0.3\n",
        "\n",
        "#     # Combine recommendations using weighted average\n",
        "#     combined_recommendations = gnn_recommendations.copy()\n",
        "#     combined_recommendations['score'] = gnn_recommendations['attr'] * gnn_weight + content_based_recommendations['score'] * content_based_weight\n",
        "#     combined_recommendations = combined_recommendations.sort_values(by='score', ascending=False)\n",
        "\n",
        "#     return combined_recommendations"
      ],
      "metadata": {
        "id": "9hC-KkhWL3Du"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hybrid_recipe_recommendation(explanation_df, recipe_fil_df, unique_recipe_id, mapped_user_id, recipe_title):\n",
        "    gnn_recommendations = recipe_recommendation_GNN(explanation_df, recipe_fil_df, unique_recipe_id, mapped_user_id)\n",
        "    # print(gnn_recommendations)\n",
        "    index = ind[recipe_title]\n",
        "    content_based_recommendations = get_food_recipe_cos_content_based(index)\n",
        "    content_based_recommendations = content_based_recommendations[['RecipeId', 'Name', 'Calories', 'RecipeCategory', 'Score']]\n",
        "    # content_based_recommendations\n",
        "    user_id = unique_user_id.loc[unique_user_id['mappedId']==mapped_user_id, 'userId'].values[0]\n",
        "\n",
        "    print(f'Recommendation of Recipe to user with id {user_id} after interacting witth \"{recipe_title}\"')\n",
        "    hybrid_df = pd.concat([gnn_recommendations, content_based_recommendations]).drop_duplicates(subset=['RecipeId'])\n",
        "    hybrid_df.fillna(0, inplace=True)\n",
        "\n",
        "\n",
        "    return hybrid_df\n"
      ],
      "metadata": {
        "id": "1mtynfoCTOt_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recommendations = hybrid_recipe_recommendation(explanation_df, recipe_fil_df, unique_recipe_id, mapped_user_id, recipe_title)\n",
        "recommendations"
      ],
      "metadata": {
        "id": "QRFdlJrJT6xD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eZgGTuGg-jhr"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "oPsNE5isSTxK",
        "qRLsYTMXwvRN",
        "FgqfK8rNlCGb"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}